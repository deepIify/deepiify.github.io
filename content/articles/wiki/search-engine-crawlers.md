---
title: 검색엔진 크롤러 (Search Engine Crawler)
description: 검색엔진 최적화를 위한 검색엔진 크롤러에 대해서 소개합니다.
color: '#000'
backgroundColor: '#b8e9ff'
img: '/images/wiki_thumb_spider.png'
createdAt: '2021-09-06 00:00:00'
updatedAt: '2021-09-06 00:00:00'
author:
  name: 엔지니어
tags: ['크롤러', '검색엔진', '검색엔진 크롤러', 'crawler', 'search engine', 'search engine crawler']
---

**크롤러**는 인터넷을 검색하고 컨텐츠를 분석하는 소프트웨어입니다. 주로 검색엔진에서 웹 사이트를 색인화하는 데 사용됩니다. 또한 웹 크롤러는 피드 또는 마케팅 컨텐츠, 이메일 주소와 같은 데이터 수집에도 사용됩니다. 크롤러는  반복적으로 웹 페이지를 자동으로 수집하는 프로그램입니다.

<!--more-->

최초의 웹 크롤러는 `World Wide Web Wanderer`라고 불리며 1993년에 인터넷의 성장을 측정하는 데 사용되었습니다. 1년 후, 최초의 인터넷 검색 엔진이 `Webcrawler`라는 이름으로 출시되어 되어 크롤러라고 불려지게 되었습니다.

크롤러는 검색엔진 최적화(SEO)와 굉장히 밀접한 관련이 있고, 성공적인 SEO를 위해서는 이러한 프로그램이 어떻게 작동하는지 알아야 합니다. 이에 대해서는 아래에서 더 자세히 소개해보도록 하겠습니다.

## 웹 크롤러의 기능

크롤러는 하이퍼링크를 통해 인터넷 검색을 하는 동안 마치 일반 사용자처럼 새로운 웹 페이지를 찾습니다. 페이지를 검색할 때 포함된 모든 URL을 저장합니다. 그런 다음 크롤러는 저장된 각 URL을 하나씩 열어 프로세스를 반복합니다. 이러한 방식으로 검색엔진은 로봇을 사용하여 웹에서 링크된 페이지를 찾습니다.

그러나 대부분의 경우 모든 URL이 크롤러에 의해 처리되지 않고 제한적으로 수집됩니다. 또한 로봇은 어느 시점에서 프로세스가 중지되었다가 다시 시작되기도하고, 수집된 정보는 일반적으로 색인으로 평가 및 저장되므로 색인을 통해 수집된 정보를 빠르게 찾을 수 있습니다.

### 사이트에서 크롤러 제어

robots 제외 표준을 사용하여 크롤러에게 웹사이트에서 색인을 생성할 페이지와 색인을 생성하지 않을 페이지를 알릴 수 있습니다. 이러한 지침은 `robots.txt`라는 파일에 선언하거나 HTML 헤더의 `메타 태그(robots)` 를 통해 전달할 수 있습니다. 하지만 모든 크롤러가 항상 이러한 지침을 따르는 것은 아니기에 주의해야합니다.

### 크롤러 솔루션의 사용 시나리오

크롤러는 광범위한 응용 프로그램들을 찾고 일반적으로 소프트웨어 패키지의 기능으로서 제공됩니다. 검색엔진과 관련된 웹 색인 외에도 프로그램을 사용하여 주제별 정보를 수집할 수도 있습니다. 웹사이트나 링크를 분류하여 크롤러의 검색이 제한되는 경우 웹에서는 주제와 관련된 페이지만 찾을 수 있습니다.

또한 크롤러는 데이터 마이닝 및 웹메트릭에 사용할 수 있습니다. 데이터 마이닝에서 로봇은 트렌드와 상호 참조를 식별하기 위해 대규모 데이터베이스에서 정보를 수집합니다. 로봇을 사용하여 관련 데이터베이스를 생성하고 평가할 수 있습니다. 반면에 웹메트릭는 컨텐츠, 속성, 구조 및 사용자 행동 측면에서 인터넷을 분석합니다.

`Harvester`는 특수한 유형의 웹 크롤러입니다. 이 용어는 웹에서 이메일 주소를 검색하고 "수집"하는 프로그램을 의미합니다. 즉, 마케팅 또는 스팸 발송과 같은 활동을 위해 목록에 저장합니다.

## 검색엔진을 위한 웹사이트 크롤링 최적화

크롤링 가능성을 최대화하고 가능한 최상의 검색엔진 최적화 결과를 얻으려면 웹사이트에 **내부링크**가 잘 연결되어 있어야 합니다. 봇은 링크를 따라 새로운 웹 페이지와 콘텐츠를 분석합니다. SEO 친화적인 링크는 검색 봇이 모든 중요한 하위 페이지를 찾을 수 있도록 합니다. 이러한 내부링크를 통해서 발견된 페이지 중 하나에서 고품질 컨텐츠가 발견되면 순위가 높을 수 있습니다.

XML 또는 HTML 사이트맵은 크롤러의 작업을 더 쉽게 만드는 일반적인 솔루션이기도 합니다. 검색 엔진이 모든 하위 페이지를 쉽게 찾고 색인할 수 있도록 웹사이트의 전체 링크 구조가 포함되어 있습니다.

SEO를 위한 HTML 태그의 올바른 사용을 과소평가해서는 안됩니다. 이러한 구조를 일관되게 사용하면 봇이 페이지 콘텐츠를 올바르게 해석하도록 도울 수 있습니다. 예를 들어 표제(h1, h2, h3 등), 링크 제목(Anchor Text) 및 이미지 설명(Alt)의 표준 사용이 포함됩니다.

Java 또는 Flash 컨텐츠를 사용해서는 안 됩니다. Google은 이제 JavaScript 페이지를 크롤링할 수 있지만 여전히 크롤링 예산이 많이 소요됩니다. 대신 PHP 또는 ASP와 같은 서버 측 언어를 사용 하여 HTML로 웹 사이트의 탐색 요소 및 기타 구성 요소를 생성해야 합니다. 클라이언트(웹 브라우저 또는 로봇)는 HTML 결과를 이해하고 색인화하기 위해 플러그인 이 필요하지 않습니다.

현대 웹사이트는 더 이상 프레임 기반이 아니라 CSS로 모든 디자인 측면을 해결해야 합니다. 오늘날에도 여전히 프레임을 사용하는 페이지는 검색 엔진에서 부분적으로만 인덱싱되고 잘못 해석됩니다.

SEO 크롤링 최적화와 관련된 또 다른 중요한 측면은 색인이 생성되어야 하는 페이지가 robots.txt의 크롤링에서 제외되거나 로봇 메타 태그에 **noindex** 지시문이 포함되어서는 안 된다는 것입니다. 여러분의 사이트 또는 웹페이지에 대해서 테스트해보고 싶다면 <nuxt-link to="/diagnosis">사이트 진단</nuxt-link>에서 확인해볼 수 있습니다.

인터넷 상에서 범죄를 위해서 사용되는 클롤링 로봇들이 있습니다. 이를 막기위해 로봇을 차단하기 위해 노력합니다. 로봇을 막기위한 구성에 중요한 Google, Naver 및 다양한 검색엔진이 포함 될 수도 있기 때문에 주의를 기울여서 잘 확인해야합니다.

마지막으로 크롤링 가능성은 웹사이트의 성능에도 영향을 받는다는 점에 유의해야 합니다. 웹 사이트가 느린 서버에 있거나 기술적인 문제로 인해 속도가 느려지면 일반적으로 좋은 검색 결과 순위를 얻지 못합니다. 페이지가 너무 오래 로드되면 봇이 크롤링을 못하기 때문에 일부 하위 페이지는 색인이 전혀 생성되지 않을 수 있습니다. 따라서 빠른 인프라는 효과적인 SEO의 기초입니다.

다음에서는 방금 설명한 사항을 간단한 체크리스트 형식으로 요약했습니다.

- 적절한 내부링크 구성
- XML 또는 HTML 사이트맵
- SEO를 위한 HTML 태그의 올바른 사용
- Java 또는 Flash 콘텐츠 제거
- 프레임 제거
- robots.txt 및 noindex에 의해 제외된 페이지 확인
- 로봇 보안의 올바른 구성
- 효과적인 SEO를 위한 빠른 성능
